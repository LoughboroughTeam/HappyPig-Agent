1. Large Language Models are Zero-Shot Reasoners: 
https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html 

2. Training Verifiers to Solve Math Word Problemsï¼š
https://arxiv.org/abs/2110.14168

3. Language Models are Few-Shot Learners:
https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html

4.Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems:
https://arxiv.org/abs/1705.04146

5. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models: 
https://arxiv.org/abs/2201.11903

6. Chain of Thought Empowers Transformers to Solve Inherently Serial Problems:
https://scholar.google.com/scholar?cluster=11728270641767049962&hl=zh-CN&as_sdt=0,5